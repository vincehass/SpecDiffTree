# SpecDiffTree MLX - Quick Test Configuration
# For Apple Silicon (M1/M2/M3/M3 Max)
# Better numerical stability than PyTorch MPS

experiment:
  name: "specdifftree_mlx_quick"
  tags: ["specdifftree", "mlx", "apple_silicon", "stage1", "quick"]
  notes: "Quick 2-epoch test with MLX on Apple Silicon"

dataset:
  name: "TSQA"
  task: "Multiple Choice Question Answering"
  num_samples_train: 6300
  num_samples_val: 630
  num_samples_test: 700

model:
  type: "OpenTSLMSP"
  llm_id: "mlx-community/Llama-3.2-1B-Instruct-4bit"
  gradient_checkpointing: false
  lora:
    enabled: false

training:
  num_epochs: 2
  batch_size: 4
  lr_encoder: 1.0e-4 # Conservative for stability
  lr_projector: 5.0e-5
  lr_base: 1.0e-4
  weight_decay: 1.0e-2
  grad_clip_norm: 1.0
  warmup_fraction: 0.05
  early_stopping_patience: 5
  patch_size: 4

device:
  type: "mlx" # Apple Silicon via MLX

evaluation:
  metrics: ["accuracy", "loss"]
  save_predictions: true

notes: |
  MLX provides better numerical stability than PyTorch MPS for LLM training.
  Should not encounter NaN loss issues that affect PyTorch MPS backend.
